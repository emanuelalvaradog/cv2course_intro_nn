{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso por gradiente\n",
    "\n",
    "En este notebook implementaremos un paso del método de descenso por gradiente. Este método nos sirve para encontrar los parámetros de la red de tal forma que la salida se asemeje a el valor objetivo.\n",
    "\n",
    "![gradiente](files/gradient_descent_1n_notebook.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/irvingvasquez/cv2course_intro_nn/blob/master/03_descenso_unpaso.ipynb)\n",
    "\n",
    "@juan1rving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos paquetes\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definimos la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# función de activación\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivada de f\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# función h lineal\n",
    "def function_h(X, W, b):\n",
    "    return np.dot(W, X) + b\n",
    "\n",
    "# Salida de la RN\n",
    "def output_y(X,W,b):\n",
    "    return sigmoid(function_h(X,W,b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Término de error\n",
    "\n",
    "Escribe una función que calcule el término de error\n",
    "\n",
    "$$\\delta= (y-\\hat{y})f' (h) = (y-\\hat{y})f' (\\sum_i w_i x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implementar el cálculo del término de error\n",
    "\n",
    "def error_term(y,W,X,b):\n",
    "    error = y - output_y(X, W, b)\n",
    "    delta = error * sigmoid_prime(output_y(X, W, b))\n",
    "    return delta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremento\n",
    "\n",
    "Escribe una función para determinar el incremento a uno de los pesos\n",
    "$$\\Delta w_i= \\eta \\delta x_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implementar el cálculo del incremento\n",
    "\n",
    "# Gradient descent is the derivative of the error function with respect to the weights.\n",
    "\n",
    "# calculus chain rule:\n",
    "# dE/dw = dE/dy * dy/dh * dh/dw\n",
    "# where dE = error term\n",
    "# dE/dy = y - y_hat\n",
    "# dy/dh = sigmoid_prime(h)\n",
    "# dh/dw = x\n",
    "\n",
    "def increment(W, X, b, eta, i, y):\n",
    "    w = eta * error_term(y, W, X, b) * X[i] \n",
    "    return w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar funcionamiento\n",
    "\n",
    "A continuación implementemos una red de ejemplo y verificaremos que está funcionando almenos un paso del método de descenso por gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores de ejemplo\n",
    "learning_rate = 0.1\n",
    "x = np.array([1,1])\n",
    "y = 5\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.1,0.2])\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida: 0.574442516811659\n",
      "Error: 4.425557483188341\n",
      "Incremento: [0.10199114047268941, 0.10199114047268941]\n",
      "Nuevos pesos: [0.20199114 0.30199114]\n",
      "Nuevo error: 4.376605275390046\n"
     ]
    }
   ],
   "source": [
    "# Calcular la salida de la red\n",
    "salida = output_y(x, w, b)\n",
    "print('Salida:', salida)\n",
    "\n",
    "# Calcula el error residual de la red\n",
    "residual = y - salida\n",
    "print('Error:', residual)\n",
    "\n",
    "# Calcula el incremento de los pesos\n",
    "incremento = [increment(w, x, b, learning_rate, 0, y), increment(w, x, b, learning_rate, 1, y)]\n",
    "print('Incremento:', incremento)\n",
    "\n",
    "# Calcula el nuevo valor del los pesos\n",
    "n_w = w + incremento\n",
    "print('Nuevos pesos:', n_w)\n",
    "\n",
    "# Calcula el nuevo error\n",
    "n_error = y - output_y(x, n_w, b)\n",
    "print('Nuevo error:', n_error)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Si el nuevo error es menor que el primer error de la red entonces nuesto método de descenso está funcionando.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
