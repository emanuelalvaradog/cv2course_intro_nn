{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso por gradiente completo\n",
    "\n",
    "En este notebook implementaremos el algoritmo completo de descenso por gradiente. Para validar que funciona al final lo probaremos en el entrenamiento de una red neuronal simple. Usaremos como conjunto de datos que esta incluído en el archivo data.csv. \n",
    "\n",
    "@juan1rving\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos los paquete necesarios\n",
    "import numpy as np\n",
    "\n",
    "# cargamos datos de ejemplo\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# En este ejercicio por propósitos de analizar las salidas utilizaremos la misma semilla para los números aleatorios.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos algunas funciones necesarias\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoide\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización de los pesos\n",
    "\n",
    "En un principio no queremos tener todos los pesos en cero porque esto generaría en la salida una predicción nula. Por lo tanto, asignaremos los pesos iniciales de forma aleatoria y cercanos a cero. Otra recomendación es escalar los valores aleatorios es dependencia del número de entradas del nodo (n).\n",
    "\n",
    "$$w = rand(1,\\frac{1}{\\sqrt{n}})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize weights. \n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.475\n"
     ]
    }
   ],
   "source": [
    "# Probemos la precisión de la red antes de entrenarla\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n",
    "\n",
    "# La precisión debe ser mala seguramente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros de la red\n",
    "\n",
    "Los hiperpámetros de la red indican el números de veces que itera el método (épocas-epochs), la taza de aprendizaje (learning rate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# número de épocas\n",
    "epochs = 50\n",
    "# tasa de aprendizaje\n",
    "learnrate = .5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descenso por gradiente completo.\n",
    "\n",
    "El algoritmo de descenso por gradiente de forma iterativa cambia el valor de los pesos de tal forma que se disminuya el error. \n",
    "\n",
    "<img src=\"files/nn_02_simple_nn_dg.png\">\n",
    "\n",
    "En la siguiete celda encontrarás la plantilla del algoritmo. Tu misión, si decides aceptarla, es completar el código faltante para que funcione el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.2627609384996635\n",
      "Train loss:  0.25623702824711486\n",
      "Train loss:  0.2505839260471835\n",
      "Train loss:  0.24564110622481955\n",
      "Train loss:  0.24128247404777373\n",
      "Train loss:  0.23741106008734986\n",
      "Train loss:  0.23395240246426385\n",
      "Train loss:  0.23084867302043308\n",
      "Train loss:  0.22805408803546176\n",
      "Train loss:  0.22553154654686108\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    # Para todos los renglones de ejemplo, asignar a x la entrada, y a y la salida deseada\n",
    "    for x, y in zip(features.values, targets):\n",
    "\n",
    "        # TODO: calcula la predicción de la red\n",
    "        h = np.dot(x, weights)\n",
    "        output = sigmoid(h)\n",
    "\n",
    "        # TODO: calcula el error\n",
    "        sigmoid_prime = output * (1 - output)\n",
    "        error = y - output\n",
    "        error_term = error * sigmoid_prime\n",
    "\n",
    "        # TODO: calcula el incremento\n",
    "        del_w += learnrate * error_term * x\n",
    "\n",
    "    # TODO: Actualiza los pesos\n",
    "    weights += 1 / len(features.values) * del_w\n",
    "\n",
    "    # Ahora calculemos el error en el conjunto de datos de entrenamiento\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluemos la exactitud de la red\n",
    "\n",
    "Recordemos que\n",
    "\n",
    "$$ Exactitud = \\frac{\\# aciertos}{\\# predicciones} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "# Cálculo de la precisión\n",
    "\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal vez obtuvimos la mejor exactidud, pero ¿qué pasará si incrementamos las épocas? O ¿qué es lo que pasará si cambiamos la tasa de aprendizaje?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbad788490f55b163920bee5e9d5e0cba00db5905dc94f4bdbe0011e55bf465f"
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
